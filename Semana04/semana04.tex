%Este trabalho está licenciado sob a Licença Creative Commons Atribuição-CompartilhaIgual 3.0 Não Adaptada. Para ver uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/3.0/ ou envie uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[../livro.tex]{subfiles}  %%DM%%Escolher document class and options article, etc

%define o diretório principal
\providecommand{\dir}{..}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%INICIO DO DOCUMENTO%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\chapter{Semana 4}


A operação de soma de matrizes e de multiplicação por escalar são definidas da mesma forma que definimos estas operações para vetores. Na soma, basta somar componente a componente, enquanto que na multiplicação por escalar o que fazemos é multiplicar todas as componentes da matriz pelo escalar (número) dado. Desta forma, por exemplo,
\begin{align*}
\left[
\begin{array}{cccc}
1 & -1 & \pi & 0 \\
11 & 11 & -2 & 0 \\
9 & 0 & 4 & 4 \\
\end{array}
\right] +
\left[
\begin{array}{cccc}
1 & 4 & 1 & 9 \\
-5 & 10 & 2 & 11 \\
-3 & 8 & 0 & -6 \\
\end{array}
\right] & =
\left[
\begin{array}{cccc}
1+1 & -1 +4 & \pi + 1 & 0+ 9 \\
11-5 & 11+10 & -2+2 & 0+11 \\
9-3 & 0+8 & 4+0 & 4-6 \\
\end{array}
\right] \\
& =
\left[
\begin{array}{cccc}
2 & 3 & \pi + 1 & 9 \\
6 & 21 & 0 & 11 \\
6 & 8 & 4 & -2 \\
\end{array}
\right]
\end{align*}
enquanto
\[
5\cdot \left[
\begin{array}{cccc}
1 & 4 & 1 & 9 \\
-5 & 10 & 2 & 11 \\
-3 & 8 & 0 & -6 \\
\end{array}
\right] =
\left[
\begin{array}{cccc}
5\cdot 1 & 5\cdot 4 & 5\cdot 1 & 5\cdot 9 \\
5\cdot (-5) & 5\cdot 10 & 5\cdot 2 & 5\cdot 11 \\
5\cdot (-3) & 5\cdot 8 & 5\cdot 0 & 5\cdot (-6) \\
\end{array}
\right] =
\left[
\begin{array}{cccc}
5 & 20 & 5 & 45 \\
-25 & 50 & 10 & 55 \\
-15 & 40 & 0 & -30 \\
\end{array}
\right].
\]


\section{Produto de matrizes}

É no produto de matrizes que aparece uma operação diferente do que estamos acostumados. Já vimos como a multiplicação de uma matriz de ordem $m\times n$ por um vetor de $\bR^n$ (ou matriz de ordem $n\times 1$) aparece naturalmente quando resolvemos uma sistema linear:
\[
A \vec{x} = \left[
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{array}
\right]
\left[
\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n} \\
\end{array}
\right] =
\left[
\begin{array}{c}
a_{11} x_{1} + a_{12} x_{2} + \cdots  a_{1n} x_{n} \\
a_{21} x_{1} + a_{22} x_{2} + \cdots  a_{2n} x_{n} \\
\vdots \\
a_{m1} x_{1} + a_{m2} x_{2} + \cdots  a_{mn} x_{n} \\
\end{array}
\right].
\] O resultado é um vetor  de $\bR^m$ (ou matriz de ordem $m\times 1$). Observamos que necessariamente o número de colunas da matriz $A$ deve ser igual ao número de linhas do vetor $\vec{x}$, para que seja possível realizar o produto como indicado.

Vamos generalizar este procedimento para definir o produto entre duas matrizes de qualquer ordem. Por exemplo, se $A$ é como acima e $B$ é uma matriz $B$ de ordem $n \times 2$, o que vamos fazer é pensar nas colunas de $B$ como dois vetores $\vec{b}_1$ e $\vec{b}_2$ e realizar os produtos como já estamos acostumados. O resultado será uma matriz cujas colunas são $A \vec{b}_1$ e $A \vec{b}_2$:
\[
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{bmatrix}
\begin{bmatrix} 
x_{1} & y_1 \\
x_{2} & y_2 \\
\vdots & \vdots \\
x_{n} & y_n \\
\end{bmatrix} =
\begin{bmatrix}
a_{11} x_{1} + a_{12} x_{2} + \cdots + a_{1n} x_{n} & a_{11} y_{1} + a_{12} y_{2} + \cdots + a_{1n} y_{n}\\
a_{21} x_{1} + a_{22} x_{2} + \cdots + a_{2n} x_{n} & a_{21} y_{1} + a_{22} y_{2} + \cdots + a_{2n} y_{n}\\
\vdots & \vdots \\
a_{m1} x_{1} + a_{m2} x_{2} + \cdots + a_{mn} x_{n} & a_{m1} y_{1} + a_{m2} y_{2} + \cdots + a_{mn} y_{n}\\
\end{bmatrix}.
\] O resultado é portanto uma matriz de ordem $m\times 2$.


De forma mais geral, podemos multiplicar uma matriz $A$ de ordem $m \times n$ por qualquer outra matriz de ordem $n \times p$ e o resultado obtido será uma matriz de ordem $m \times p$. É fácil de lembrar: o número de colunas da primeira matriz deve ser igual ao número de linhas da segunda matriz e a matriz resultante terá por ordem o que  ``sobrar'':
\[
[\cdots]_{m \times \cancel{n}} \, [\cdots]_{\cancel{n} \times p} = [\cdots]_{m \times p}.
\] As colunas da matriz resultante são obtidas ao fazer o produto da primeira matriz com os vetores que formam as colunas da segunda:
\[
A B =
\left[
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{array}
\right]
\left[
\begin{array}{cccc}
| & |  &   & | \\
\vec{b}_{1} & \vec{b}_{2} & \cdots & \vec{b}_{p} \\
| & |  &   & | \\
\end{array}
\right] =
\left[
\begin{array}{cccc}
| & |  &   & | \\
A\vec{b}_{1} & A\vec{b}_{2} & \cdots & A\vec{b}_{p} \\
| & |  &   & | \\
\end{array}
\right].
\] Acima, $\vec{b}_{1}, \vec{b}_{2}, \dots, \vec{b}_{p} \in \bR^n$ são as colunas da matriz $B$. Conclua que $A\vec{b}_{1}, A\vec{b}_{2}, \dots, A\vec{b}_{p} \in \bR^m$ e que $AB$ é uma matriz de ordem $m \times p$.

\subsection{Exemplos}

Vamos mostrar algumas contas que exemplificam tanto o procedimento quanto algumas das propriedades essenciais do produto de matrizes.


\begin{example} Vamos calcular um produto de matrizes utilizando o procedimento explicado na subseção anterior:
	\[
	\left[
	\begin{array}{cccc}
	1 & -1 & \pi & 0 \\
	11 & 11 & -2 & 0 \\
	\end{array}
	\right] \cdot
	\left[
	\begin{array}{cccc}
	1 & 4 & 1 & 9 \\
	-5 & 10 & 2 & 11 \\
	-3 & 8 & 0 & -6 \\
	0 & 2 & -2 & 0 \\
	\end{array}
	\right]  =
	\left[
	\begin{array}{cccc}
	6-3\pi   & -6 + 8\pi &   -1   &  -2 - 6 \pi  \\
	50      &  138      &  33    &  232         \\
	\end{array}
	\right].
	\] O elemento $(AB)_{ij}$ da linha $i$ e coluna $j$ da matriz $AB$, pode ser obtido ao ``multiplicar''\footnote{O leitor já familiarizado com geometria analítica pode identificar o elemento $ij$ da matriz resultante como o produto escalar entre a linha $i$ de $A$ e a coluna $j$ de $B$.} a linha $i$ de $A$ pela coluna $j$ de B:
	\[
	\begin{array}{ccccccccccc}
	(AB)_{11} & = & 1\cdot 1 & + & (-1)\cdot(-5)  & + & \pi \cdot (-3) & + & 0\cdot 0    & = & 6-3\pi \\
	(AB)_{12} & = & 1\cdot 4 & + & (-1)\cdot 10   & + & \pi \cdot 8    & + & 0\cdot 2    & = & -6 + 8 \pi \\
	(AB)_{13} & = & 1\cdot 1 & + & (-1)\cdot 2    & + & \pi \cdot 0    & + & 0\cdot (-2) & = & -1 \\
	(AB)_{14} & = & 1\cdot 9 & + & (-1)\cdot 11   & + & \pi \cdot (-6) & + & 0\cdot 0    & = & -2-6\pi \\
	(AB)_{21} & = & 11\cdot 1& + & 11\cdot(-5)    & + & (-2)\cdot(-3)  & + & 0\cdot 0    & = & 50 \\
	(AB)_{22} & = & 11\cdot 4& + & 11\cdot 10     & + & (-2)\cdot 8    & + & 0\cdot 2    & = & 138 \\
	(AB)_{23} & = & 11\cdot 1& + & 11\cdot 2      & + & (-2)\cdot 0    & + & 0\cdot (-2) & = & 33 \\
	(AB)_{24} & = & 11\cdot 9& + & 11\cdot 11     & + & (-2)\cdot (-6) & + & 0\cdot 0    & = & 232. \\
	\end{array}
	\] Observamos, a partir deste exemplo, que \textbf{o produto de matrizes não é comutativo}. De fato, as matrizes $AB$ e $BA$ sequer tem a mesma ordem. Além disto, pode ser que seja possível calcular $AB$, mas não $BA$. Pense, por exemplo, em uma matriz $A$ de ordem $3 \times 2$ e em outra matriz $B$ de ordem $2 \times 5$.
\end{example}




\begin{example}
	Uma propriedade de \textbf{matrizes quadradas}, isto é, matrizes cujo número de linhas e de colunas é igual, é que o produto delas fornece uma terceira matriz com a mesma ordem:
	\[
	[\cdots]_{n \times \cancel{n}} \, [\cdots]_{\cancel{n} \times n} = [\cdots]_{n \times n}.
	\] Mesmo neste caso, o produto de matrizes \textbf{pode não ser comutativo}: é possível encontrar matrizes tais que $AB \neq BA$. Por exemplo,
	\[
	\left[
	\begin{array}{cc}
	1 & -1  \\
	0 &  1  \\
	\end{array}
	\right]
	\left[
	\begin{array}{cc}
	1 & -1  \\
	1 &  2  \\
	\end{array}
	\right] =
	\left[
	\begin{array}{cc}
	0 & -3  \\
	1 &  2  \\
	\end{array}
	\right]
	\] enquanto
	\[
	\left[
	\begin{array}{cc}
	1 & -1  \\
	1 &  2  \\
	\end{array}
	\right]
	\left[
	\begin{array}{cc}
	1 & -1  \\
	0 &  1  \\
	\end{array}
	\right] =
	\left[
	\begin{array}{cc}
	1 & 0  \\
	1 & -1  \\
	\end{array}
	\right]. \ \lhd
	\]
\end{example}





\subsection{Uma interpretação para resolução de sistemas lineares}\label{scn:resolucao-2-sistemas}

Nós podemos pensar em
\[
\left[
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{array}
\right]
\left[
\begin{array}{cc}
x_{1} & y_1 \\
x_{2} & y_2 \\
\vdots & \vdots \\
x_{n} & y_n \\
\end{array}
\right] =
\left[
\begin{array}{cc}
b_1 & c_1 \\
b_2 & c_2 \\
\vdots & \vdots \\
b_{n} & c_{n}\\
\end{array}
\right]
\] como a forma matricial de escrever dois sistemas lineares que possuem a mesma matriz associada:
\[
A =
\left[
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\
\end{array}
\right]: \quad A \vec{x} = \vec{b}, \quad A \vec{y} = \vec{c}.
\] (por quê?) É possível de resolver os dois sistemas concomitantemente ao escrever uma matriz aumentada generalizada:
\[
\left[
\begin{array}{cccc|cc}
a_{11} & a_{12} & \cdots & a_{1n} & b_1 & c_1 \\
a_{21} & a_{22} & \cdots & a_{2n} & b_2 & c_2\\
\vdots & \vdots &        & \vdots  & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} & b_{n} & c_{n}\\
\end{array}
\right]
\] A análise das soluções é feita da mesma forma que aprendemos nas notas da primeira semana. Observem que esta é uma forma de economizar as contas e não precisar escalonar duas vezes a matriz $A$ associada!

\begin{exercise}
	Escrever dois sistemas lineares que possuem a mesma matriz associada. Resolver usando a matriz aumentada generalizada acima.
\end{exercise}

Evidentemente, este método funcionaria para resolver ao mesmo tempo 3 ou 4 ou 20 sistemas lineares ao mesmo tempo, desde que a matriz associada seja a mesma em todos eles.



\section{Matriz transposta}


A \textbf{matriz transposta} de uma matriz $A$, de ordem $m\times n$, é a matriz $A^T$ que tem por colunas as linhas de $A$. Consequentemente, $A^T$ é uma matriz de ordem $n \times m$.

\begin{example}
	As transpostas das matrizes
	\[
	A = \left[
	\begin{array}{c}
	-1  \\
	10  \\
	-9  \\
	\end{array}
	\right], \quad
	B = \left[
	\begin{array}{cc}
	1 & -1  \\
	2 & 11 \\
	\end{array}
	\right], \quad
	C = \left[
	\begin{array}{cccc}
	1  & -1 & \pi & 0 \\
	11 & 11 & -2  & 0 \\
	9  & 4  & 4   & 4 \\
	\end{array}
	\right]
	\] são, respectivamente, as matrizes
	\[
	A^T = \left[
	\begin{array}{ccc}
	-1 & 10 & 9  \\
	\end{array}
	\right], \quad
	B^T = \left[
	\begin{array}{cc}
	1  & 2  \\
	-1 & 11 \\
	\end{array}
	\right], \quad
	C^T = \left[
	\begin{array}{ccc}
	1   & 11 & 9 \\
	-1  & 11 & 4 \\
	\pi & -2 & 4 \\
	0  & 0  & 4 \\
	\end{array}
	\right]. \ \lhd
	\]
\end{example}


Valem as seguintes propriedades:
\begin{itemize}
	\item $(A^T)^T = A$
	\item Linearidade: $(xA + y B)^T = x A^T + y B^T$
	\item Produto: $(AB)^T = B^T A^T$
\end{itemize}

\begin{exercise}
	Verifique que estas propriedades são válidas em alguns exemplos (escolha algumas matrizes para fazer as contas).
\end{exercise}



\section{Matriz inversa}

Agora que temos definido um produto de matrizes, é natural de nos perguntarmos quais das propriedades usuais do produto entre números reais se mantém válidas.

Por exemplo, a \textbf{matriz identidade} $I_n$ é a matriz quadrada de ordem $n \times n$ que tem $1$ na diagonal principal e $0$ nas demais posições. No caso $3 \times 3$, temos
\[
I_3 =
\left[
\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{array}
\right].
\] Esta matriz satisfaz $A I_n = A$ para qualquer matriz $A$ de ordem $m \times n$. Da mesma forma, temos $I_n B = B$ para qualquer matriz $B$ de ordem $n \times m$ (observe atentamente os índices!). O nome identidade vem desta propriedade, que é parecida com a propriedade da unidade nos números reais: $1\cdot x = x = x\cdot 1$.

Existindo a matriz identidade, outra pergunta natural é se existe um inverso multiplicativo. Para números reais, qualquer número não nulo possui:
\[
x \in \bR, x \neq 0 \implies x^{-1} = \frac{1}{x} \ \text{é um inverso multiplicativo,}
\] isto é, satisfaz $x\cdot x^{-1} = 1$.

Vamos procurar por \textbf{matrizes inversas}: dada uma matriz $A$, queremos encontrar uma matriz $A^{-1}$ de modo que
\[
A A^{-1} = I_n = A^{-1} A.
\] Escrevemos de duas formas diferentes acima, pois o produto de matrizes não é comutativo. A matriz $A^{-1}$ é chamada a \textbf{matriz inversa} de $A$. Observe que  \textbf{$A$ deve ser quadrada} (por quê?).

\subsection{Caso $2\times 2$}

Vamos procurar pela inversa de
\[
A = \left[
\begin{array}{cc}
a & b  \\
c & d \\
\end{array}
\right].
\] Escrevemos
\[
A^{-1} = \left[
\begin{array}{cc}
x_1 & y_1  \\
x_2 & y_2 \\
\end{array}
\right]
\] e queremos descobrir os valores $x_1, x_2, y_1, y_2$ que satisfazem
\[
\left[
\begin{array}{cc}
a & b  \\
c & d \\
\end{array}
\right]
\left[
\begin{array}{cc}
x_1 & y_1  \\
x_2 & y_2 \\
\end{array}
\right] =
\left[
\begin{array}{cc}
1 & 0 \\
0 & 1 \\
\end{array}
\right]
\] Pela interpretação que demos anteriormente ao produto de matrizes, encontrar
\[
\vec{x} = \left[
\begin{array}{c}
x_1   \\
x_2  \\
\end{array}
\right] \quad \text{e} \quad
\vec{y} = \left[
\begin{array}{c}
y_1  \\
y_2 \\
\end{array}
\right]
\] é equivalente a resolver ao mesmo tempo os dois sistemas
\[
A \vec{x} =
\left[
\begin{array}{c}
1  \\
0 \\
\end{array}
\right] = \vec{e}_1 \quad \text{e} \quad
A \vec{y} =
\left[
\begin{array}{c}
0  \\
1 \\
\end{array}
\right] = \vec{e}_2.
\] A ideia é então resolver por escalonamento os sistemas cuja matriz aumentada associada é:
\[
\left[
\begin{array}{cc|cc}
a & b & 1 & 0 \\
c & d & 0 & 1 \\
\end{array}
\right].
\] Tem-se
\[
\left[
\begin{array}{cc|cc}
a & b & 1 & 0 \\
c & d & 0 & 1 \\
\end{array}
\right] \xrightarrow{c\cdot \ell_1 \text{ e } a\cdot \ell_2}
\left[
\begin{array}{cc|cc}
ac & bc & c & 0 \\
ac & ad & 0 & a \\
\end{array}
\right] \xrightarrow{-\ell_1 + \ell_2 \text{ em } \ell_2}
\left[
\begin{array}{cc|cc}
ac & bc      & c  & 0 \\
0 & ad - bc & -c & a \\
\end{array}
\right]
\]
\[
\xrightarrow{\ell_2 \div (ad-bc)}
\left[
\begin{array}{cc|cc}
ac & bc      & c  & 0 \\
0 & 1 & \frac{-c}{ad - bc} & \frac{a}{ad - bc} \\
\end{array}
\right]  \xrightarrow{-bc\ell_2 + \ell_1 \text{ em } \ell_1}
\left[
\begin{array}{cc|cc}
ac & 0 & \frac{acd}{ad - bc} & \frac{-abc}{ad - bc} \\
0 & 1 & \frac{-c}{ad - bc}  & \frac{a}{ad - bc} \\
\end{array}
\right]
\]
\[
\xrightarrow{\ell_1 \div ac}
\left[
\begin{array}{cc|cc}
1 & 0 & \frac{ d}{ad - bc}  & \frac{-b}{ad - bc} \\
0 & 1 & \frac{-c}{ad - bc}  & \frac{ a}{ad - bc} \\
\end{array}
\right].
\] Daí concluimos (depois colocar em evidência o fator $ad - bc$ que está dividindo) que:
\[
\boxed{A^{-1} = \frac{1}{ad - bc} \left[
	\begin{array}{cc}
	d  & -b  \\
	-c & a \\
	\end{array}
	\right] =
	\frac{1}{\det A} \left[
	\begin{array}{cc}
	d  & -b  \\
	-c & a \\
	\end{array}
	\right].}
\] Nesta última expressão, definimos o \textbf{determinante} de uma matriz de ordem $2\times 2$:
\[
\det A = ad - bc
\] e, como foi necessário dividir por este valor, concluimos que:
\[
\boxed{\text{só existe a matriz inversa de $A$ caso $\det A \neq 0$.}}
\]


\begin{remark}
	Veremos na seção seguinte que o processo que utilizamos para inverter a matriz $A$ funciona para matrizes de qualquer ordem, mas é trabalhoso. No caso de uma matriz $2\times 2$, talvez seja interessante memorizar a fórmula, já que não é tão complicada. Podemos pensar da seguinte maneira:
	\begin{itemize}
		\item Calculamos $\det A$. Se for diferente de zero, existe a inversa. Nós partimos de $A$ e dividimos pelo determinante.
		\item Em seguida, trocamos de lugar os elementos da diagonal principal ($a$ e $d$).
		\item Finalmente, trocamos o sinal dos elementos da outra diagonal.
	\end{itemize}
	\textbf{Atenção!} Este método apenas funciona para matrizes quadradas de ordem $2\times 2$!
\end{remark}

\begin{example}\label{exp:2x2}
	Sejam as matrizes
	\[
	A = \left[
	\begin{array}{cc}
	1 & 1  \\
	-1 & 3 \\
	\end{array}
	\right], \quad
	B = \left[
	\begin{array}{cc}
	1 & 0  \\
	1 & 1 \\
	\end{array}
	\right], \quad
	C = \left[
	\begin{array}{cc}
	-1 & -1  \\
	2 & 2 \\
	\end{array}
	\right].
	\] Calculamos
	\[
	\det A = 1\cdot 3 - (-1)\cdot 1 = 4 \neq 0.
	\]
	Logo, $A$ possui inversa e temos
	\[
	A^{-1} = \frac{1}{4}
	\left[
	\begin{array}{cc}
	3 & -1  \\
	1 & 1 \\
	\end{array}
	\right] =
	\left[
	\begin{array}{cc}
	3/4 & -1/4  \\
	1/4 & 1/4 \\
	\end{array}
	\right].
	\](trocamos de lugar os elementos da diagonal principal e de sinal os elementos da outra diagonal).
	
	Façamos o mesmo para a matriz $B$:
	\[
	\det B = 1\cdot 1 - 1 \cdot 0 = 1 \neq 0.
	\] Logo,
	\[
	B^{-1} = \frac{1}{1}
	\left[
	\begin{array}{cc}
	1 & -1  \\
	1 & 1 \\
	\end{array}
	\right] =
	\left[
	\begin{array}{cc}
	3/4 & -1/4  \\
	1/4 & 1/4 \\
	\end{array}
	\right].
	\](trocamos de lugar os elementos da diagonal principal -- neste caso eram iguais -- e de sinal dos elementos da outra diagonal).
	
	Já para a matriz $C$, temos
	\[
	\det C = (-1) \cdot 2 - 2 \cdot (-1) = 0
	\] e portanto $C$ não é invertível (isto é, não existe a matriz inversa $C^{-1}$)$. \ \lhd$
\end{example}




\subsection{Algoritmo para ordem maior}\label{scn:alg-ordem-maior}

Considere uma matriz de ordem $n \times n$:
\[
A =
\left[
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{array}
\right].
\] Para obter a matriz inversa, devemos ter:
\[
\left[
\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots &        & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn} \\
\end{array}
\right]
\left[
\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1n} \\
x_{21} & x_{22} & \cdots & x_{2n} \\
\vdots & \vdots &        & \vdots \\
x_{n1} & x_{n2} & \cdots & x_{nn} \\
\end{array}
\right]
=
\left[
\begin{array}{cccc}
1   &    0   & \cdots &    0   \\
0   &    1   & \cdots &    0   \\
\vdots & \vdots &        & \vdots \\
0   &    0   & \cdots &    1   \\
\end{array}
\right].
\] Da mesma forma que na seção anterior, isto é equivalente a resolver simultaneamente os sistemas:
\[
A \vec{x}_1 = \vec{e}_1, \quad A \vec{x}_2 = \vec{e}_2, \dots, A \vec{x}_n = \vec{e}_n,
\] onde $\vec{x}_1, \vec{x}_2, \dots, \vec{x}_n$ são as colunas da matriz inversa $A^{-1}$. Assim, devemos escrever a matriz aumentada associada a estes sistemas lineares:
\[
\left[
\begin{array}{cccc|cccc}
a_{11} & a_{12} & \cdots & a_{1n} &   1   &    0   & \cdots &    0 \\
a_{21} & a_{22} & \cdots & a_{2n} &   0   &    1   & \cdots &    0\\
\vdots & \vdots &        & \vdots &   \vdots & \vdots &        & \vdots  \\
a_{m1} & a_{m2} & \cdots & a_{mn} &   0   &    0   & \cdots &    1\\
\end{array}
\right]
\] ou, de forma mais sucinta,
\[
\big[ \, A \ | \ I \ \big].
\] E este é o algoritmo de resolução: Deixar a matriz $A$ em forma escalonada reduzida (até chegar na matriz identidade) de modo que as soluções obtidas já serão a matriz inversa de $A$:
\[
\big[ \, I \ | \ A^{-1} \ \big].
\]

\begin{remark}
	Para que seja possível encontrar as soluções, como indicado acima, a forma escalonada da matriz $A$ deve possuir $n$ posições de pivô (caso contrário, algum dos sistemas acima seria impossível), de modo que todas as colunas de $A$ são colunas pivô.
	
	Se $A$ possuir alguma coluna sem posição de pivô, podemos parar imediatamente o algoritmo, pois isto significa que a matriz não é invertível.
\end{remark}


\begin{example}
	Vamos decidir se a matriz abaixo é invertível e, em caso afirmativo, determinar a inversa:
	\[
	A =
	\left[
	\begin{array}{cccc}
	2 & 1 & 1 & 0 \\
	4 & 3 & 3 & 1 \\
	8 & 7 & 9 & 5 \\
	6 & 7 & 9 & 8 \\
	\end{array}
	\right].
	\] De acordo com o algoritmo, devemos escalonar
	\[
	\left[
	\begin{array}{cccc|cccc}
	2 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
	4 & 3 & 3 & 1 & 0 & 1 & 0 & 0 \\
	8 & 7 & 9 & 5 & 0 & 0 & 1 & 0 \\
	6 & 7 & 9 & 8 & 0 & 0 & 0 & 1 \\
	\end{array}
	\right].
	\] Caso a matriz seja invertível, conseguiremos deixar a matriz identidade do lado esquerdo desta matriz aumentada. Começamos eliminando os termos da primeira coluna (abaixo do $2$ que está na posição de pivô):
	\[
	\left[
	\begin{array}{cccc|cccc}
	2 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
	0 & 1 & 1 & 1 & -2 & 1 & 0 & 0 \\
	0 & 3 & 5 & 5 & -4 & 0 & 1 & 0 \\
	0 & 4 & 6 & 8 & -3 & 0 & 0 & 1 \\
	\end{array}
	\right] \xrightarrow{\text{segunda coluna}}
	\left[
	\begin{array}{cccc|cccc}
	2 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
	0 & 1 & 1 & 1 & -2 & 1 & 0 & 0 \\
	0 & 0 & 2 & 2 & 2 & -3 & 1 & 0 \\
	0 & 0 & 2 & 4 & 5 & -4 & 0 & 1 \\
	\end{array}
	\right]
	\]
	\[
	\xrightarrow{\text{terceira}}
	\left[
	\begin{array}{cccc|cccc}
	2 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
	0 & 1 & 1 & 1 & -2 & 1 & 0 & 0 \\
	0 & 0 & 2 & 2 & 2 & -3 & 1 & 0 \\
	0 & 0 & 0 & 2 & 3 & -1 & -1 & 1 \\
	\end{array}
	\right]
	\xrightarrow{\text{reduzida -- }4^a}
	\left[
	\begin{array}{cccc|cccc}
	2 & 1 & 1 & 0 & 1 & 0 & 0 & 0 \\
	0 & 1 & 1 & 0 & -7/2 & 3/2 & 1/2 & -1/2 \\
	0 & 0 & 1 & 0 & -1/2 & -1 & 1 & -1/2 \\
	0 & 0 & 0 & 2 & 3 & -1 & -1 & 1 \\
	\end{array}
	\right]
	\]
	\[
	\to
	\left[
	\begin{array}{cccc|cccc}
	2 & 1 & 0 & 0 & 3/2 & 1 & -1 & 1/2 \\
	0 & 1 & 0 & 0 & -3 & 5/2 & -1/2 & 0 \\
	0 & 0 & 1 & 0 & -1/2 & -1 & 1 & -1/2 \\
	0 & 0 & 0 & 1 & 3/2 & -1/2 & -1/2 & 1/2 \\
	\end{array}
	\right]\to
	\left[
	\begin{array}{cccc|cccc}
	1 & 0 & 0 & 0 & 9/4 & -3/4 & -1/4 & 1/4 \\
	0 & 1 & 0 & 0 & -3 & 5/2 & -1/2 & 0 \\
	0 & 0 & 1 & 0 & -1/2 & -1 & 1 & -1/2 \\
	0 & 0 & 0 & 1 & 3/2 & -1/2 & -1/2 & 1/2 \\
	\end{array}
	\right].
	\] Concluimos que
	\[
	A^{-1} =
	\left[
	\begin{array}{cccc}
	9/4 & -3/4 & -1/4 & 1/4 \\
	-3 & 5/2 & -1/2 & 0 \\
	-1/2 & -1 & 1 & -1/2 \\
	3/2 & -1/2 & -1/2 & 1/2 \\
	\end{array}
	\right].
	\] Verifique como exercício que esta matriz é de fato a matriz inversa de $A$, isto é, calcule o produto $A \cdot A^{-1}$ e verifique que resulta em $I_4$. 
	
	Notamos que, caso nosso único interesse fosse decidir se $A$ é invertível, sem necessariamente calcular sua inversa, poderíamos ter feito o escalonamento de $A$ (nem precisa ser da matriz aumentada $\big[ \, I \ | \ A^{-1} \ \big]$) e parado o processo quando chegamos em
	\[
	A \sim \left[
	\begin{array}{cccc}
	2 & 1 & 1 & 0 \\
	0 & 1 & 1 & 1 \\
	0 & 0 & 2 & 2 \\
	0 & 0 & 0 & 2 \\
	\end{array}
	\right],
	\] pois já está claro que todas as colunas possuem posição de pivô. $\ \lhd$
\end{example}



\begin{example}
	Vamos decidir se a matriz abaixo é invertível e, em caso afirmativo, determinar a inversa:
	\[
	A =
	\left[
	\begin{array}{cccc}
	1 & 1 & 1  \\
	1 & 3 & -3  \\
	-3 & -9 & 9  \\
	\end{array}
	\right].
	\] De acordo com o algoritmo, devemos escalonar
	\[
	\left[
	\begin{array}{ccc|ccc}
	1 & 1 & 1 & 1 & 0 & 0  \\
	1 & 3 & -3 & 0 & 1 & 0  \\
	-3 & -9 & 9  & 0 & 0 & 1 \\
	\end{array}
	\right].
	\] Temos
	\[
	\left[
	\begin{array}{ccc|ccc}
	1 & 1 & 1 & 1 & 0 & 0  \\
	0 & 2 & -4 & -1 & 1 & 0  \\
	0 & -6 & 12  & 3 & 0 & 1 \\
	\end{array}
	\right] \to
	\left[
	\begin{array}{ccc|ccc}
	1 & 1 & 1 & 1 & 0 & 0  \\
	0 & 2 & -4 & -1 & 1 & 0  \\
	0 & 0 & 0  & 0 & 3 & 1 \\
	\end{array}
	\right].
	\] Portanto, a terceira coluna não possui posição de pivô e a matriz $A$ não possui inversa.
\end{example}


\subsection{Aplicação na resolução de sistemas lineares}


Sistemas lineares de ordem $n \times n$
\[
A \vec{x} = \vec{b},
\] cuja matriz associada $A$ é invertível, são sistemas que possuem exatamente uma solução, para qualquer vetor $\vec{b} \in \bR^n$ (ver Subseção \ref{sec:inversa}).

A existência da matriz inversa $A^{-1}$ permite-nos multiplicar ambos os lados do sistema por $A^{-1}$ para obter $\vec{x} = A^{-1} \cdot A = A^{-1} \vec{b}.$ Logo,
\[
\boxed{\vec{x} = A^{-1} \vec{b}.}
\]


\begin{example}
	Resolver o sistema
	\[
	A = \left[
	\begin{array}{cc}
	1 & 1  \\
	-1 & 3 \\
	\end{array}
	\right] \left[
	\begin{array}{c}
	x_1  \\
	x_2 \\
	\end{array}
	\right] =
	\left[
	\begin{array}{c}
	1  \\
	2 \\
	\end{array}
	\right].
	\] Já calculamos a matriz inversa no Exemplo \ref{exp:2x2}:
	\[
	A^{-1}  =
	\left[
	\begin{array}{cc}
	3/4 & -1/4  \\
	1/4 & 1/4 \\
	\end{array}
	\right].
	\] Segue que a solução é
	\[
	\vec{x} = A^{-1} \vec{b} =
	\left[
	\begin{array}{cc}
	3/4 & -1/4  \\
	1/4 & 1/4 \\
	\end{array}
	\right]\left[
	\begin{array}{c}
	1  \\
	2 \\
	\end{array}
	\right] =
	\left[
	\begin{array}{c}
	1/4 \\
	3/4 \\
	\end{array}
	\right]. \ \lhd
	\]
\end{example}


Embora nossa solução do sistema possa parecer ``elegante'' ao utilizar a matriz inversa, este método é muito ineficiente. Na verdade, escalonar a matriz aumentada associada ao sistema exige um custo computacional muito menor do que calcular a inversa da matriz e, portanto, em geral se usa o escalonamento.

Matrizes inversas têm uma importância mais teórica no nosso curso, como veremos na subseção abaixo.



\subsection{Uma primeira caracterização de matrizes invertíveis}\label{sec:inversa}


Vamos, nesta subseção, verificar (provar de forma rigorosa) que
% \[
% \boxed{
% 	\begin{split}
% 	&\text{A matriz $A$ é invertível se, e somente se, o sistema linear $A \vec{x} = \vec{b}$ possui}  \\
% 	&\text{exatamente uma solução, para qualquer vetor $\vec{b} \in \bR^n$.}
% 	\end{split}
% }
% \]
\begin{center}
  ``a matriz $A$ é invertível se, e somente se, o sistema linear $A \vec{x} = \vec{b}$ possui exatamente uma solução, para qualquer vetor $\vec{b} \in \bR^n$.''
\end{center}

\noindent $(\!\implies\!\!)$ Se $A$ é invertível, então conseguimos resolver todos os sistemas
\[
A \vec{x}_1 = \vec{e}_1, \quad A \vec{x}_2 = \vec{e}_2, \cdots, A \vec{x}_n = \vec{e}_n
\] concomitantemente. De fato, qualquer vetor $\vec{b}$ pode ser escrito como
\[
\vec{b} =  
\left[
\begin{array}{c}
b_1 \\
b_2 \\
\vdots \\
b_n \\
\end{array}
\right] = 
b_1 \vec{e}_1 + b_2 \vec{e}_2 + \cdots + b_n \vec{e}_n,
\] e daí verificamos que se pode construir uma solução $\vec{x}$ pela fórmula
\[
\vec{x} = b_1 \vec{x}_1 + b_2 \vec{x}_2 + \cdots + b_n \vec{x}_n,
\] já que pela linearidade do produto da matriz $A$ por vetores, temos
\begin{align*}
A  \vec{x} & = b_1 A\vec{x}_1 + b_2 A\vec{x}_2 + \cdots + b_n A\vec{x}_n, \\
& = b_1 \vec{e}_1 + b_2 \vec{e}_2 + \cdots + b_n \vec{e}_n \\
& = \vec{b}.
\end{align*}

\noindent $(\!\Longleftarrow)$ Reciprocamente, suponhamos que o sistema possua exatamente uma solução, para qualquer vetor $\vec{b} \in \bR^n$. Em particular, podemos resolver os sitemas
\[
A \vec{x}_1 = \vec{e}_1, \quad A \vec{x}_2 = \vec{e}_2, \cdots, A \vec{x}_n = \vec{e}_n
\] e escrever a matriz inversa de acordo com o nosso algoritmo:
\[
A^{-1} = 
\begin{bmatrix}
\ |  &  |  &   & | \ \\
\ \vec{x}_1  &  \vec{x}_2 & \cdots & \vec{x}_n \ \\
\ |  &  |  &   & | \ \\
\end{bmatrix}.
\]



\end{document} 