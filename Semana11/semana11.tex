%Este trabalho está licenciado sob a Licença Creative Commons Atribuição-CompartilhaIgual 3.0 Não Adaptada. Para ver uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/3.0/ ou envie uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[../livro.tex]{subfiles}  %%DM%%Escolher document class and options article, etc

%define o diretório principal
\providecommand{\dir}{..}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%INICIO DO DOCUMENTO%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\chapter{Semana 11}


Nesta semana, nós começamos uma abordagem um pouco mais geométrica de espaços vetoriais: ângulo entre vetores, comprimento de vetores, ortogonalidade. No entanto, nosso objetivo é também o de entender como estes conceitos se comportariam em dimensões quaisquer (inclusive maiores do que $3$).

\section{Comprimento, ângulos e o produto escalar}

Se, na base canônica (isto é, no sistema de coordenadas usual) do espaço $\bR^3$, representamos um vetor
\[
\vec{v} =
\begin{bmatrix}
v_1 \\ v_2 \\ v_3
\end{bmatrix},
\] o seu \textbf{comprimento} (ou \textbf{magnitude} ou \textbf{norma} de $\vec{v}$) é dado por
\[
\|\vec{v}\| = \sqrt{v_1^2 + v_2^2 + v_3^2}.
\]
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.3\linewidth]{\dir/Semana11/semana11-coord}
	\end{center}
\end{figure}

\noindent Esta definição pode ser generalizada para qualquer dimensão:
\[
\vec{v} =
\begin{bmatrix}
v_1 \\ v_2 \\ \vdots \\ v_n
\end{bmatrix} \quad \rightsquigarrow \quad \|\vec{v}\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
\]


O \textbf{produto escalar} ou \textbf{produto interno} entre dois vetores $\vec{v}$ e $\vec{w}$ é um número (também dito escalar) associado com estes dois vetores. Em coordenadas, se $\vec{v} = (v_1, v_2, v_3)$ e $\vec{w} = (w_1, w_2, w_3)$, temos
\[
\vec{v} \cdot \vec{w} = v_1 w_1 + v_2 w_2 + v_3 w_3.
\] Vejamos que realmente o produto escalar tem a ver com o ângulo entre dois vetores. Isto é uma consequência da Lei dos Cossenos, que nos diz que
\[
\|\vec{v} - \vec{w}\|^2 = \|\vec{v}\|^2 + \|\vec{w}\|^2 - 2 \|\vec{v}\| \|\vec{w}\| \cos \theta,
\] onde $\theta$ é o ângulo entre $\vec{v}$ e $\vec{w}$. Escrevendo estes comprimentos em termos das coordenadas e fazendo os cancelamentos necessários, chegamos na seguinte identidade:
\[
v_1 w_1 + v_2 w_2 + v_3 w_3 = \|\vec{v}\| \|\vec{w}\| \cos \theta, \quad \text{ isto é,} \quad \vec{v} \cdot \vec{w} = \|\vec{v}\| \|\vec{w}\| \cos \theta.
\]


Motivados pelos conceitos acima, para
\[
\vec{x} =
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix} \quad \vec{y} =
\begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}
\] vetores de $\bR^n$ representados na base canônica, definimos o \textbf{produto escalar} ou \textbf{produto interno} entre $\vec{x}$ e $\vec{y}$:
\[
\vec{x} \cdot \vec{y} = x_1 y_1 + x_2 y_2 + \cdots + x_n y_n.
\] O ângulo entre $\vec{x}$ e $\vec{y}$, no caso $n \le 4$, pode ser \textit{definido} pela equação
\[
\cos \theta = \frac{\vec{x} \cdot \vec{y}}{\|\vec{x}\| \|\vec{y}\|}.
\]

\begin{proposition}[Principais propriedades do produto escalar]
	Para $\vec{x}, \vec{y}, \vec{z} \in \bR^n$ e $c \in \bR$ um escalar, temos as seguintes propriedades
	\begin{enumerate}
		\item  $\vec{x} \cdot \vec{y} = \vec{y} \cdot \vec{x}$
		\item  $(\vec{x} + \vec{y}) \cdot \vec{z} = \vec{x} \cdot \vec{z} + \vec{y} \cdot \vec{z}$
		\item  $(c\vec{x}) \cdot \vec{y} = c \vec{x} \cdot \vec{y} = \vec{x} \cdot (c\vec{y})$
		\item  $\vec{x} \cdot \vec{x} \ge 0$ e $\, \vec{x} \cdot \vec{x} = 0$ se, e somente se, $\vec{x} = \vec{0}$;
		\item  $\|\vec{x}\| = \sqrt{\vec{x} \cdot \vec{x}}$;
		\item  $\| c \vec{x} \| =  |c| \| \vec{x} \|$.
	\end{enumerate}
\end{proposition}

\begin{example}
	Considere os vetores de $\bR^5$ escritos na base canônica:
	\[
	\vec{x} =
	\begin{bmatrix}
	-1 \\ 2 \\ 0 \\ 3 \\ -1
	\end{bmatrix} \quad \vec{y} =
	\begin{bmatrix}
	4 \\ 3 \\ -1 \\ -11 \\ 1
	\end{bmatrix}
	\] Assim, temos
	\[
	\vec{x} \cdot \vec{y} = (-1)\cdot 4 + 2 \cdot 3 + 0 \cdot (-1) +3 \cdot(-11) + (-1)\cdot 1 = -4 + 6 +0 -33 -1 = -31.
	\] Da mesma forma, podemos calcular os comprimentos
	\[
	\| \vec{x} \| = \sqrt{(-1)^2 + 2^2 + 0^2 + 3^2 + (-1)^2} = \sqrt{15} \quad \text{e} \quad \| \vec{y} \| = \sqrt{4^2 + 3^2 + (-1)^2 + (-11)^2 + 1^2} = \sqrt{148}.
	\] Logo, o ângulo $\theta$ entre estes vetores satisfaz (utilizamos uma calculadora para calcular as raízes quadradas e a função inversa do cosseno):
	\[
	\cos \theta = \frac{\vec{x} \cdot \vec{y}}{\|\vec{x}\| \|\vec{y}\|} = \frac{-31}{\sqrt{15} \, \sqrt{148}} \simeq -0,65794 \implies  \theta \simeq 2,28887634 \textrm{ rad} \simeq 131,12^o.
	\]
\end{example}

\begin{example}
	Considere um vetor tridimensional
	\[
	\vec{u} =
	\begin{bmatrix}
	2 \\ 3 \\ -1
	\end{bmatrix}.
	\] Já sabemos a multiplicação de um escalar pelo vetor $\vec{u}$ muda o comprimeto (e talvez também o sentido) do vetor. Por exemplo, se multiplicarmos por $2$, o comprimento será multiplicado por $2$; se dividirmos por $3$, o comprimento será dividido por $3$. Se nós dividirmos pelo próprio comprimento de $\vec{u}$ nós obtemos um vetor unitário. Para verificar analíticamente, basta utilizar a propriedade $6$ acima:
	\[
	c = \frac{1}{\|\vec{u}\|} \implies \left\| \frac{\vec{u}}{\|\vec{u}\|} \right\| = \|c \vec{u} \| = c \|\vec{u}\| = \frac{1}{\|\vec{u}\|}\|\vec{u}\| = 1.
	\] Então se quisermos encontrar um vetor unitário na direção de $\vec{u}$ acima, calculamos
	\[
	\|\vec{u}\| = \sqrt{4 + 9 + 1} = \sqrt{14} \quad \rightsquigarrow \quad \vec{v} = \frac{\vec{u}}{\|\vec{u}\|} = \frac{\vec{u}}{\sqrt{14}} =
	\begin{bmatrix}
	2/\sqrt{14} \\ 3/\sqrt{14} \\ -1/\sqrt{14}
	\end{bmatrix}
	\] é o vetor procurado. Este processo de obter um vetor unitário na mesma direção de um vetor previamente fixado é chamado de \textbf{normalização}. Observe que apenas existem dois vetores unitários na mesma direção de $\vec{u}$, a saber: $\vec{v}$ e $- \vec{v}$.
\end{example}


\section{Ortogonalidade}

Como vimos na seção anterior, o produto escalar está relacionado com o ângulo entre dois vetores pela fórmula
\[
\cos \theta = \frac{\vec{x} \cdot \vec{y}}{\|\vec{x}\| \|\vec{y}\|}.
\] Quando este ângulo $\theta$ é igual a $\pi /2$ (que é equivalente a $90^o$), vale que $\cos \theta = 0$ e logo $\vec{x} \cdot \vec{y} = 0$.

Nós vamos dizer que os dois vetores $\vec{x}$ e $\vec{y}$ são \textbf{ortogonais} quando satisfazem
\[
\vec{x} \cdot \vec{y} = 0.
\] Poderíamos também usar a palavra perpendiculares, mas por algum motivo esta terminologia é menos comum quando lidamos com vetores em espaços vetoriais de dimensão maior do que $3$.

Dizemos também que um conjunto de vetores é um \textbf{conjunto ortogonal} se todo par de vetores do conjunto for ortogonal. Em outras palavras, um conjunto $\{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\}$ é um conjunto ortonogonal se, para qualquer escolha de índices $i \neq j$, tivermos $\vec{v}_i \cdot \vec{v}_j = 0$.


\begin{example}
	Os vetores
	\[
	\vec{x} = \begin{bmatrix}
	-1 \\ 2
	\end{bmatrix} \quad \text{e} \quad
	\vec{y} = \begin{bmatrix}
	2 \\ 1
	\end{bmatrix}
	\] são ortogonais, pois $\vec{x} \cdot \vec{y} = (-1)\cdot 2 + 2 \cdot 1 = 0$, enquanto que os vetores
	\[
	\vec{w} = \begin{bmatrix}
	1 \\ 2
	\end{bmatrix} \quad \text{e} \quad
	\vec{z} = \begin{bmatrix}
	2 \\ 1
	\end{bmatrix}
	\] não são ortogonais, já que $\vec{w} \cdot \vec{z} = 1\cdot 2 + 2 \cdot 1 = 4$. Nestes casos, podemso dizer que o conjunto $\{\vec{x}, \vec{y}\}$ é um conjunto ortogonal enquanto $\{\vec{w}, \vec{z}\}$ não é.
\end{example}



\begin{example}
	A base canônica do espaço $\bR^n$:
	\[
	\vec{e}_1 =
	\begin{bmatrix}
	1 \\ 0 \\ \vdots \\ 0
	\end{bmatrix}, \
	\vec{e}_2 =
	\begin{bmatrix}
	0 \\ 1 \\ \vdots \\ 0
	\end{bmatrix}, \ \cdots,
	\vec{e}_n =
	\begin{bmatrix}
	0 \\ 0 \\ \vdots \\ 1
	\end{bmatrix}
	\] é um conjunto ortogonal, porque quando escolhermos $i\neq j$, temos
	\[
	\vec{e}_i \cdot \vec{e}_j = 1 \cdot 0 + 0 \cdot 1 = 0. \ \lhd
	\]
\end{example}

Uma primeira propriedade é uma versão do Teorema de Pitágoras, que aparece naturamente ao considerarmos vetores ortogonais.

\begin{theorem}[Teorema de Pitágoras]
	Se dois vetores $\vec{x}$ e $\vec{y}$ são ortogonais, então
	\[
	\|\vec{x} + \vec{y}\|^2 = \|\vec{x}\|^2 + \|\vec{y}\|^2.
	\]
\end{theorem}

\begin{proof}
	Esperamos que esta demonstração auxilie no compreendimento analítico e geométrico dos conceitos que introduzimos até agora. O vetor $\vec{x} + \vec{y} \,$ pode ser considerado a hipotenusa do triângulo retângulo de catetos $\vec{x}$ e $\vec{y}$ (ver figura).
	
	Pelas propriedades do comprimento e do produto escalar, temos que, para \textit{quaisquer} dois vetores $\vec{x}$ e $\vec{y}$ (não necessariamente ortogonais), vale que
	\[
	\|\vec{x} + \vec{y}\|^2 = (\vec{x} + \vec{y}) \cdot (\vec{x} + \vec{y}) =  \vec{x} \cdot \vec{x} + \vec{x} \cdot \vec{y} +  \vec{y} \cdot \vec{x} +  \vec{y} \cdot \vec{y} = \|\vec{x}\|^2 + 2 (\vec{x} \cdot \vec{y}) + \|\vec{y}\|^2.
	\] De certa maneira, esta é uma forma de reescrever a Lei dos Cossenos. Quando os vetores $\vec{x}$ e $\vec{y}$ forem ortogonais, temos $\vec{x} \cdot \vec{y} = 0$, concluimos que
	\[
	\|\vec{x} + \vec{y}\|^2 = \|\vec{x}\|^2 + \|\vec{y}\|^2. \qedhere
	\]
\end{proof}


Notamos que, de acordo com a nossa definição, o vetor nulo é ortogonal a qualquer outro. Vejamos, no entanto, que

\begin{theorem}
	Todo conjunto ortogonal $\{\vec{v}_1, \vec{v}_2, \dots, \vec{v}_k\}$ formado por vetores \textit{não nulos} é um conjunto linearmente independente (LI).
\end{theorem}

\begin{proof}
	Considerando uma combinação linear
	\[
	c_1 \vec{v}_1 + c_2 \vec{v}_2 + \dots + c_k \vec{v}_k = \vec{0}
	\] e fazendo o produto escalar com qualquer dos vetores $\vec{v}_j$ na equação acima, concluimos que
	\[
	(c_1 \vec{v}_1 + c_2 \vec{v}_2 + \dots + c_k \vec{v}_k ) \cdot \vec{v}_j = \vec{0} \cdot \vec{v}_j.
	\] Agora, a condição de ortogonalidade nos diz que $\vec{v}_i \cdot \vec{v}_j = 0$ sempre que $i \neq j$. Portanto,
	\[
	c_j \| \vec{v}_j \| = c_j \vec{v}_j \cdot \vec{v}_j = 0 \stackrel{\vec{v}_j \neq \vec{0}}{\implies} c_j = 0.
	\] Isto é a definição do conjunto ser LI: qualquer combinação linear que resulte no vetor nulo deve ter todos os coeficientes nulos.
\end{proof}

\vspace{0.2cm}

Uma das aplicações mais importantes do produto escalar reside no fato de nós conseguirmos fazer projeções ortogonais em direções fixadas. Por exemplo, suponhamos que em um sistema físico, um vetor $\vec{v}$ representa a força aplicada em um ponto, como na figura.
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\linewidth]{\dir/Semana11/semana11-proj}
	\end{center}
\end{figure}

\noindent Gostaríamos de encontrar a componente do vetor na direção da reta tracejada. Para isto, podemos considerar um vetor \textit{unitário} $\vec{u}$ sobre a reta. Assim, a componente de $\vec{v}$ na dereção de $\vec{u}$ é igual a $k \vec{u}$, onde $k$ tem a ver com o cosseno do ângulo entre os vetores:
\[
\cos \theta = \frac{\text{cateto adjacente}}{\text{hipotenusa}} = \frac{k}{\|\vec{v}\|}.
\] Utilizando novamente que $\vec{u}$ é um vetor unitário, concluimos que
\[
k = \|\vec{v}\| \cos \theta = \|\vec{u}\| \|\vec{v}\| \cos \theta = \vec{u} \cdot \vec{v}.
\] Assim sendo, definimos a \textbf{projeção ortogonal de $\vec{v}$ em um vetor unitário $\vec{u}$} como sendo
\[
\boxed{\proj_{\vec{u}} \vec{v} = (\vec{u} \cdot \vec{v}) \, \vec{u}.}
\] No \textbf{caso onde $\vec{u}$ não está normalizado}, devemos primeiramente normalizar o vetor, de modo que a projeção ortogonal é dada por
\[
\boxed{\proj_{\vec{u}} \vec{v} = \bigg( \frac{\vec{u}}{\|\vec{u}\|} \cdot \vec{v} \bigg) \, \frac{\vec{u}}{\|\vec{u}\|} = \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\|^2} \, \vec{u} = \frac{\vec{u} \cdot \vec{v}}{\vec{u} \cdot \vec{u}} \, \vec{u}.}
\]

\begin{example}\label{canon}
	Considere vetores no espaço de quatro dimensões $\bR^4$. Vamos determinar a projeção do vetor $\vec{v} =
	\begin{bmatrix}
	1 \\ -1 \\ 3 \\ 4
	\end{bmatrix}$ na direção da reta que tem a mesma direção do vetor $\vec{u} =
	\begin{bmatrix}
	1 \\ 1 \\ 1 \\ -1
	\end{bmatrix}.$ Observamos inicialmente que $\|\vec{u}\| = \sqrt{4} = 2$ não é unitário. Temos que
	\[
	\proj_{\vec{u}} \vec{v} =  \frac{\vec{u} \cdot \vec{v}}{\|\vec{u}\|^2} \, \vec{u} = \frac{1 - 1 + 3 - 4}{4}
	\begin{bmatrix}
	1 \\ 1 \\ 1 \\ -1
	\end{bmatrix} =
	-\frac{1}{4}
	\begin{bmatrix}
	1 \\ 1 \\ 1 \\ -1
	\end{bmatrix} =
	\begin{bmatrix}
	1/4 \\ 1/4 \\ 1/4 \\ -1/4
	\end{bmatrix}.
	\] A projeção de $\vec{v}$ sobre o vetor $\vec{e}_3$ da base canônica de $\bR^4$ é (observe que $\|\vec{e}_3\| = 1$)
	\[
	\proj_{\vec{e}_3} \vec{v} = (\vec{e}_3 \cdot \vec{v}) \, \vec{e}_3 = (0 + 0 + 3 + 0)
	\begin{bmatrix}
	0 \\ 0 \\ 1 \\ 0
	\end{bmatrix} = \begin{bmatrix}
	0 \\ 0 \\ 3 \\ 0
	\end{bmatrix}.
	\] De forma geral, podemos escrever
	\[
	\vec{v} = \big( \vec{v} \cdot \vec{e}_1 \big) \vec{e}_1 + \big( \vec{v} \cdot \vec{e}_2 \big) \vec{e}_2  + \big( \vec{v} \cdot \vec{e}_3 \big) \vec{e}_3  + \big( \vec{v} \cdot \vec{e}_4 \big) \vec{e}_4.
	\] Este tipo de represetação será explorado com mais detalhes na próxima seção.
\end{example}

As projeções ortogonais também podem ser utilizadas para o cálculo de distâncias de ponto à retas que passam pela origem: Pela interpretação geométrica da subtração de vetores, obtemos um vetor perpendicular à reta com a direção de $\vec{u}$ e que termina onde termina o vetor $\vec{v}$ (ver figura).
\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=1\linewidth]{\dir/Semana11/semana11-dist}
	\end{center}
\end{figure}

\noindent Desta maneira, o comprimento deste vetor, a saber $\|\vec{u} - \proj_{\vec{u}} \vec{v}\|$, é a distância procurada.\footnote{Lembre: a distância de um ponto a uma reta é a menor distância entre este ponto e os pontos da reta; por esta razão que procuramos um vetor perpendicular à reta.}

\begin{example}
	Calcular a distância do ponto $P = (0,2,3)$ e a reta que passa pela origem e tem a direção do vetor $\vec{u} =
	\begin{bmatrix}
	1 \\ 2 \\1
	\end{bmatrix}.$
	
	Para isto, consideramos o vetor $\vec{v}$ cujas componentes são as componentes do ponto $P$. Assim, estamos na situação da discussão acima e vale que
	\[
	\proj_{\vec{u}} \vec{v} = \frac{7}{6}
	\begin{bmatrix}
	1 \\ 2 \\1
	\end{bmatrix} \implies
	\vec{u} - \proj_{\vec{u}} \vec{v} =
	\begin{bmatrix}
	0 \\ 2 \\ 3
	\end{bmatrix} - \frac{7}{6}
	\begin{bmatrix}
	1 \\ 2 \\1
	\end{bmatrix} =
	\begin{bmatrix}
	-7/6 \\ -1/3 \\ 11/6
	\end{bmatrix}.
	\] Portanto,
	\[
	d = \|\vec{u} - \proj_{\vec{u}} \vec{v}\| = \frac{\sqrt{49 + 1 + 121}}{6} = \frac{\sqrt{171}}{6} \simeq 2,18.
	\]
\end{example}

\begin{exercise}
	Como poderíamos fazer se a reta não passasse pela origem? O mesmo raciocínio funciona?
\end{exercise}


\section{Bases ortogonais e ortonormais}


Vimos que uma das importâncias do produto escalar é estender o conceito de ortogonalidade para espaços vetoriais mais gerais. A ortogonalidade traz consigo a noção de projeção ortogonal. Agora vamos ver algumas vantagens em se considerar bases de espaços vetoriais que são também conjuntos ortogonais; estas são conhecidas como \textbf{bases ortogonais}.

\begin{example}\label{ortonormal}
	A base canônica de $\bR^n$ forma um conjunto ortogonal e é, portanto, uma base ortogonal. Assim como no Exemplo \ref{canon}, é possível representar qualquer vetor $\vec{v}$ como
	\[
	\vec{v} = \big( \vec{v} \cdot \vec{e}_1 \big) \vec{e}_1 + \big( \vec{v} \cdot \vec{e}_2 \big) \vec{e}_2  + \cdots  + \big( \vec{v} \cdot \vec{e}_n \big) \vec{e}_n.
	\]
\end{example}

Vejamos que esta propriedade da base canônica é, na realidade, uma propriedade de todas as bases ortogonais:

\begin{theorem}\label{ortog}
	Seja $\cB = \{ \vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\}$ uma base ortogonal de $\bR^n$. Então qualquer vetor $\vec{v} \in \bR^n$ pode ser representado como
	\[
	\vec{v} = \frac{\vec{v} \cdot \vec{v}_1}{\vec{v}_1 \cdot \vec{v}_1} \, \vec{v}_1 + \frac{\vec{v} \cdot \vec{v}_2}{\vec{v}_2 \cdot \vec{v}_2} \, \vec{v}_2  + \cdots  + \frac{\vec{v} \cdot \vec{v}_n}{\vec{v}_n \cdot \vec{v}_n} \, \vec{v}_n.
	\] Podemos escrever, de outra maneira, que
	\[
	\vec{v} = \big( \proj_{\vec{v}_1} \vec{v} \big) \, \vec{v}_1 + \big( \proj_{\vec{v}_2} \vec{v} \big) \, \vec{v}_2  + \cdots  + \big( \proj_{\vec{v}_n} \vec{v} \big) \, \vec{v}_n.
	\]
\end{theorem}

Este resultado mostra que bases ortogonais são especiais e boas de trabalhar. Lembra que anteriormente, para encontrar os coeficientes de um vetor em uma base, tínhamos que encontrar $c_1, c_2, \cdots, c_n$ resolvendo o sistema linear cuja forma vetorial é
\[
c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_n \vec{v}_n = \vec{v}.
\] O teorema acima afirma, em outras palavras, quando a base é ortogonal, os coeficientes do vetor $\vec{v}$ na base $\cB$ são os números
\[
c_j = \frac{\vec{v} \cdot \vec{v}_j}{\vec{v}_j \cdot \vec{v}_j} = \proj_{\vec{v}_j} \vec{v}.
\]

\begin{proof}[Justificativa do Teorema \ref{ortog}]
	Se tivermos que
	\[
	\vec{v} = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_n \vec{v}_n,
	\] então, fazendo o produto escalar desta expressão com $\vec{v}_j$ e utilizando as propriedades de ortogonalidade, concluimos que
	\[
	\vec{v} \cdot \vec{v}_j = \big( c_1 \vec{v}_1 + c_2 \vec{v}_2 + \cdots + c_n \vec{v}_n \big) \cdot \vec{v}_j = c_1 \vec{v}_1\cdot \vec{v}_j + c_2 \vec{v}_2\cdot \vec{v}_j + \cdots + c_n \vec{v}_n\cdot \vec{v}_j = c_j \big( \vec{v}_j \cdot \vec{v}_j \big),
	\] pois $\vec{v}_i \cdot \vec{v}_j = 0$ sempre que $i \neq j$.
\end{proof}

\begin{example}
	Vamos justificar que os vetores
	\[
	\vec{u} =
	\begin{bmatrix}
	4 \\ 2
	\end{bmatrix} \ \ \text{ e }\ \
	\vec{v} =
	\begin{bmatrix}
	-1 \\ 2
	\end{bmatrix}
	\] formam uma base ortogonal para $\bR^2$.
	
	Em primeiro lugar, formam uma base, pois são vetores linearmente independentes e são dois (que é a dimensão de $\bR^2$). Agora, calculamos
	\[
	\vec{u} \cdot \vec{v} = 4\cdot (-1) + 2\cdot 2 = 0.
	\] Segue que $\vec{u}$ e $\vec{v}$ são também ortogonais e, logo, formam uma base ortogonal de $\bR^2$.
	
	Para escrever um vetor $\vec{x} =
	\begin{bmatrix}
	2 \\ -7
	\end{bmatrix}$ nesta base, podemos calcular as projeções nos elementos da base (isto apenas vale se os elementos da base são ortogonais!):
	\[
	\vec{x} = \frac{\vec{x} \cdot \vec{u}}{\vec{u} \cdot \vec{u}} \, \vec{u} + \frac{\vec{x} \cdot \vec{v}}{\vec{v} \cdot \vec{v}} \, \vec{v} = \frac{-6}{20} \, \vec{u} + \frac{-16}{5} \, \vec{v} = -0.3 \, \vec{u} -3.2 \, \vec{v}.
	\]
\end{example}


Observe no exemplo acima que é muito mais fácil obter as componentes por ortogonalidade do que resolver sistemas lineares. Isto mesmo em dimensão baixa! Imagine se fosse um sistema $5\times 5$ ou ainda maior.


Note que a base canônica, como no Exemplo \ref{ortonormal}, tem a propriedade adicional que $\|\vec{e}_j\| = 1$ para todo índice $j$. Isto fez com que a representação de vetores do Teorema \ref{ortog} assumisse um formato ainda mais simples. Estas bases ortogonais que tem por elementos vetores unitários são conhecidas como \textbf{bases ortonormais}. Mais explicitamente, uma base ortonormal é uma base $\{\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\}$ de um espaço vetorial que adicionalmente satisfaz
\[
\vec{v}_i \cdot \vec{v}_j = 0 \text{ para } i \neq j \text{ e também } \|\vec{v}_j\| = 1 \text{ para todo } j \ \ \big( \iff \vec{v}_j \cdot \vec{v}_j = 1 \big)
\] Uma consequência é que, sendo $\{\vec{v}_1, \vec{v}_2, \cdots, \vec{v}_n\}$ uma base ortonormal de $\bR^n$, podemos escrever
\[
\vec{v} = \big( \vec{v} \cdot \vec{v}_1 \big) \vec{v}_1 + \big( \vec{v} \cdot \vec{v}_2 \big) \vec{v}_2  + \cdots + \big( \vec{v} \cdot \vec{v}_n \big) \vec{v}_n.
\]

\begin{example}
	Os vetores
	\[
	\vec{u} =
	\begin{bmatrix}
	1 \\ 0 \\ 0
	\end{bmatrix}, \
	\vec{v} =
	\begin{bmatrix}
	0 \\ \sqrt{2}/2 \\ \sqrt{2}/2
	\end{bmatrix}, \ \text{e }
	\vec{w} =
	\begin{bmatrix}
	0 \\ \sqrt{2}/2 \\ - \sqrt{2}/2
	\end{bmatrix}
	\] formam uma base ortonormal de $\bR^3$: verifique! Qualquer vetor $\vec{x} \in \bR^3$ pode ser escrito como 
	\[
	\vec{x} = \big( \vec{x} \cdot \vec{u} \big) \vec{u} + \big( \vec{x} \cdot \vec{v} \big) \vec{v} + \big( \vec{x} \cdot \vec{w} \big) \vec{w}
	\] Por exemplo,
	\[
	\vec{x} =
	\begin{bmatrix}
	2 \\ -1 \\ 1
	\end{bmatrix} = 2 \vec{u} + 0 \vec{v} - \sqrt{2} \vec{w} = 2 
	\begin{bmatrix}
	1 \\ 0 \\ 0
	\end{bmatrix} + \sqrt{2} 
	\begin{bmatrix}
	0 \\ \sqrt{2}/2 \\ - \sqrt{2}/2
	\end{bmatrix}.
	\]
\end{example}


Observamos finalmente que bases ortogonais podem ser facilmente transformadas em bases ortonormais pelo processo de normalização.


\begin{example}
	Os vetores 
	\[
	\vec{u} =
	\begin{bmatrix}
	1 \\ 1 \\ 1
	\end{bmatrix}, \
	\vec{v} =
	\begin{bmatrix}
	1 \\ 0 \\ -1
	\end{bmatrix} \ \text{e }
	\vec{w} =
	\begin{bmatrix}
	1 \\ -2 \\ 1
	\end{bmatrix}
	\] formam uma base ortogonal de $\bR^3$. Normalizando cada um dos vetores, podemos obter uma base ortonormal de $\bR^3$:
	\[
	\left\{
	\begin{array}{ll}
	\|\vec{u}\| = \sqrt{1 + 1 + 1} = \sqrt{3} \\
	\|\vec{v}\| = \sqrt{1 + 1} = \sqrt{2} \\
	\|\vec{w}\| = \sqrt{1 + 4 + 1} = \sqrt{6} \\
	\end{array}
	\right.
	\] Isto implica que 
	\[
	\frac{\vec{u}}{\|\vec{u}\|} = \frac{1}{\sqrt{3}}
	\begin{bmatrix}
	1 \\ 1 \\ 1
	\end{bmatrix} = 
	\begin{bmatrix}
	1/\sqrt{3} \\ 1/\sqrt{3} \\ 1/\sqrt{3}
	\end{bmatrix}, \
	\frac{\vec{v}}{\|\vec{v}\|} = \frac{1}{\sqrt{2}}
	\begin{bmatrix}
	1 \\ 0 \\ -1
	\end{bmatrix} =
	\begin{bmatrix}
	1/\sqrt{2} \\ 0 \\ -1/\sqrt{2}
	\end{bmatrix} \ \text{e }
	\frac{\vec{w}}{\|\vec{w}\|} = \frac{1}{\sqrt{6}}
	\begin{bmatrix}
	1 \\ -2 \\ 1
	\end{bmatrix} = 
	\begin{bmatrix}
	1/\sqrt{6} \\ -2/\sqrt{6} \\ 1/\sqrt{6}
	\end{bmatrix}
	\] forma um base ortonormal de $\bR^3$.
\end{example}



\end{document} 