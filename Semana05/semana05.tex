%Este trabalho está licenciado sob a Licença Creative Commons Atribuição-CompartilhaIgual 3.0 Não Adaptada. Para ver uma cópia desta licença, visite http://creativecommons.org/licenses/by-sa/3.0/ ou envie uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[../livro.tex]{subfiles}  %%DM%%Escolher document class and options article, etc

%define o diretório principal
\providecommand{\dir}{..}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%INICIO DO DOCUMENTO%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\chapter{Semana 5}

\section{Interpretações de sistemas lineares e de matrizes invertíveis}

Uma das belezas da Álgebra Linear é que diversos conceitos podem ser interpretados de maneiras diferentes. Assim, propriedades intuitivas de uma formulação de um certo problema podem ser traduzidas para outras formulações onde estas propriedades nem sempre são tão aparentes.

Um sistema linear pode ser reescrito de diversas maneiras.
\begin{enumerate}[1.]
	\item \textbf{Sistema linear}:
	\[
	\left\{
	\begin{array}{rl}
	7x_1 - 3x_2 & = 2 \\
	3x_1 - x_2 + x_3 & = 0 \\
	x_2 + 2x_3 & = -2
	\end{array}
	\right..
	\] Aparece geralmente na formulação ou na modelagem do problema. As variáveis $x_1, x_2, x_3$ indicam as variáveis que estamos interessados em analisar e o problema específico no fornece os coeficientes do sistema.
	\item \textbf{Matriz aumentada associada}:
	\[
	\left[
	\begin{array}{ccc|c}
	7 & -3 & 0 & 2 \\
	3 & -1 & 1 & 0 \\
	0 & 1 & 2 & -2 \\
	\end{array}
	\right]
	\] Esta forma é adequada para resolver o sistema por escalonamento (também conhecido como método de eliminação gaussiana). Poderíamos fazer eliminação gaussiana sem recorrer à matriz aumentada associada, mas isto implicaria em ter que ficar reescrevendo as variáveis independentes a todo momento, além de tomar certo cuidado para não se confundir com os coeficientes nulos. O escalonamento de uma matriz torna o procedimento mais automatizado e minimiza as chances de erro.
	\item  \textbf{Forma matricial}:
	\[
	\left[
	\begin{array}{ccc}
	7 & -3 & 0  \\
	3 & -1 & 1  \\
	0 & 1 & 2  \\
	\end{array}
	\right]
	\left[
	\begin{array}{c}
	x_1   \\
	x_2   \\
	x_3   \\
	\end{array}
	\right] =
	\left[
	\begin{array}{c}
	2   \\
	0   \\
	-2  \\
	\end{array}
	\right]  \quad \text{ ou } \quad A \vec{x} = \vec{b}.
	\] Nesta forma, aparece o produto de uma matriz por um vetor. Depois podemos considerar produto de matrizes e a resolução de sistemas lineares concomitantemente (ver também capítulo da Semana 04). Caso a matriz $A$ acima seja invertível, sabemos que o sistema possui apenas uma solução e que
	\[
	\vec{x} =  A^{-1} \vec{b}.
	\]
	\item  \textbf{Forma vetorial} ou \textbf{combinação linear das colunas}:
	\[
	x_1 \left[
	\begin{array}{ccc}
	7   \\
	3   \\
	0   \\
	\end{array}
	\right] + x_2
	\left[
	\begin{array}{c}
	-3   \\
	-1   \\
	1   \\
	\end{array}
	\right] + x_3
	\left[
	\begin{array}{c}
	0  \\
	1  \\
	2  \\
	\end{array}
	\right] =
	\left[
	\begin{array}{c}
	2  \\
	0  \\
	-2  \\
	\end{array}
	\right]
	\] Aqui, há uma interpretação mais geométrica. Estudar a existência de soluções do sistema linear é equivalente a se perguntar se o vetor
	\[
	\vec{b} =
	\left[
	\begin{array}{c}
	2  \\
	0  \\
	-2  \\
	\end{array}
	\right]
	\] pode ser escrito como combinação linear dos vetores
	\[
	\vec{v}_1 = \left[
	\begin{array}{ccc}
	7   \\
	3   \\
	0   \\
	\end{array}
	\right], \quad \vec{v}_2 =
	\left[
	\begin{array}{c}
	-3   \\
	-1   \\
	1   \\
	\end{array}
	\right] \quad \text{e} \quad \vec{v}_3 =
	\left[
	\begin{array}{c}
	0  \\
	1  \\
	2  \\
	\end{array}
	\right],
	\] que são as colunas da matriz $A$. Logo, resolver o sistema linear é equivalente a perguntar se $\vec{b}$ está no espaço gerado por $\vec{v}_1, \vec{v}_2$ e $\vec{v}_3$, isto é, se $\vec{b} \in \Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$.
	\item \textbf{Transformações lineares}: Decidir se $\vec{b}$ pertence à imagem da transformação linear $T: \bR^3 \to \bR^3$, definida por:
	\[
	T(x_1, x_2, x_3) = (7x_1 - 3x_2, 3x_1 - x_2 + x_3, x_2 + 2x_3).
	\] Apesar de esta ser apenas uma outra maneira de escrever a forma matricial, ela tem uma interpretação diferente, já que traz a linguagem de teoria de funções para o contexto de Álgebra Linear. De fato, vemos que a matriz canônica associada a $T$ é $A$, de modo que
	\[
	T(\vec{x}) = A \vec{x}
	\] e os conceitos introduzidos anteriormente podem ser traduzidos de forma natural. Por exemplo, combinando as interpretações das formas matricial e vetorial, podemos dizer que são equivalentes:
	\begin{itemize}
		\item existir solução do sistema linear $A \vec{x} = \vec{b}$;
		\item vetor $\vec{b}$ pertencer ao conjunto gerado pelas colunas da matriz $A$;
		\item vetor $\vec{b}$ pertencer à imagem da transformação linear $T(\vec{x}) = A \vec{x}$.
	\end{itemize}
\end{enumerate}


Vamos analisar dois sistemas lineares, com todas as ferramentas que estudamos até agora e com dois objetivos principais: familiarizar ainda mais os leitores com os conceitos até então introduzidos e preparar o caminho para o resultado da subseção seguinte sobre matrizes invertíveis.

Os exemplos abaixo são de sistemas lineares cuja matriz associada é \textbf{quadrada}, já que estamos estudando matrizes invertíveis, que devem ser quadradas. De qualquer maneira, alguns raciocínios podem ser adaptados para quaisquer sistemas (quais?).

\begin{example}
	Seja o sistema linear dado acima, cuja matriz aumentada associada é
	\[
	\left[
	\begin{array}{ccc|c}
	7 & -3 & 0 & 2 \\
	3 & -1 & 1 & 0 \\
	0 & 1 & 2 & -2 \\
	\end{array}
	\right].
	\] Nossa técnica básica de escalonamento permite fazer uma análise de todos os pontos de vista acima. Os primeiros passos do escalonamento podem ser:
	\[
	\xrightarrow{\ell_2 \leftrightarrow \ell_3}
	\left[
	\begin{array}{ccc|c}
	7 & -3 & 0 & 2 \\
	0 & 1 & 2 & -2 \\
	3 & -1 & 1 & 0 \\
	\end{array}
	\right]\xrightarrow{-(3/7)\ell_1 + \ell_3 \text{ em } \ell_3}
	\left[
	\begin{array}{ccc|c}
	7 & -3 & 0 & 2 \\
	0 & 1 & 2 & -2 \\
	0 & 2/7 & 1 & -6/7 \\
	\end{array}
	\right]
	\]
	\[
	\xrightarrow{7 \cdot \ell_3}
	\left[
	\begin{array}{ccc|c}
	7 & -3 & 0 & 2 \\
	0 & 1 & 2 & -2 \\
	0 & 2 & 7 & -6 \\
	\end{array}
	\right] \xrightarrow{-2\ell_2 + \ell_3 \text{ em } \ell_3}
	\left[
	\begin{array}{ccc|c}
	7 & -3 & 0 & 2 \\
	0 & 1 & 2 & -2 \\
	0 & 0 & 3 & -2 \\
	\end{array}
	\right]
	\] Estas continhas iniciais já revelam muito! Por exemplo:
	\begin{itemize}
		\item Chegamos à forma escalonada e não há linha do tipo $[0 \ 0 \ 0 \ | \ 1]$; logo, o sistema possui solução;
		\item De fato, podemos dizer mais: já que todas as colunas da matriz $A$ possuem posição de pivô, existe apenas uma solução;
		\item Podemos dizer mais ainda: já que todas as colunas da matriz $A$ possuem posição de pivô, o sistema possuiria solução, quaisquer que fossem os números do lado direito da igualdade, isto é, as coordenadas do vetor $\vec{b}$;
		\item Todas as colunas da matriz $A$ possuem posição de pivô e portanto a forma escalonada reduzida da matriz $A$ será a matriz identidade $I_3$. Em particular, seria possível obter a matriz inversa a $A$, desde que começássemos escalonando a matriz $[A \ |  I_3 ]$.
	\end{itemize} Agora, vejamos como poderíamos reescrever estas conclusões utilizando os conceitos estudados nas semanas anteriores:
	\begin{itemize}
		\item Para todo $\vec{b} \in \bR^3$, o sistema $A \vec{x} = \vec{b}$ possui exatamente uma solução;
		\item Todo vetor $\vec{b} \in \bR^3$ pode ser escrito como combinação linear das colunas $\vec{v}_1, \vec{v}_2, \vec{v}_3$ de $A$;
		\item Todo vetor $\vec{b} \in \bR^3$ pertence a $\Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$, espaço gerado pelas colunas de $A$;
		\item $\Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\} = \bR^3$;
		\item $A$ é equivalente por linhas à matriz identidade $I_3$;
		\item $A$ é invertível;
		\item A imagem da transformação linear $T(\vec{x}) = A \vec{x}$ é todo o espaço $\bR^3$, isto é, $T$ é sobrejetora;
		\item A equação $T(\vec{x}) = \vec{b}$ possui no máximo uma solução para cada $\vec{b} \in \bR^3$ (de fato, possui exatamente uma!), de modo que $T$ é injetora;
		\item $T$ é invertível, já que é ao mesmo tempo injetora e sobrejetora.
	\end{itemize} Todas estas conclusões vieram de alguns passos que levaram a matriz $A$ à sua forma escalonada. Como veremos adiante, ainda mais coisas poderiam ser ditas! $\ \lhd$
\end{example}



\begin{example}
	Analisamos agora o sistema linear cuja matriz aumentada assciada é
	\[
	\left[
	\begin{array}{ccc|c}
	1 & -3 & 0 & 2 \\
	0 &  4 & 1 & -3 \\
	3 & -1 & 2 & 0 \\
	\end{array}
	\right].
	\] Nossa técnica básica de escalonamento permite fazer uma análise de todos os pontos de vista acima. Os primeiros passos do escalonamento podem ser:
	\[
	\xrightarrow{-3\ell_1 + \ell_3 \text{ em } \ell_3}
	\left[
	\begin{array}{ccc|c}
	1 & -3 & 0 & 2 \\
	0 &  4 & 1 & -3 \\
	0 &  8 & 2 & -6 \\
	\end{array}
	\right] \xrightarrow{-2\ell_2 + \ell_3 \text{ em } \ell_3}
	\left[
	\begin{array}{ccc|c}
	1 & -3 & 0 & 2 \\
	0 &  4 & 1 & -3 \\
	0 &  0 & 0 & 0 \\
	\end{array}
	\right].
	\] Assim como no exemplo anterior, estas contas também revelam toda a estrutura do sistema linear e de sua matriz associada.
	\begin{itemize}
		\item Este sistema possui infinitas soluções, pois possui uma variável livre;
		\item Nem todas as colunas da matriz $A$ possuem posição de pivô. Consequentemente, escolhas diferentes do vetor $\vec{b}$, poderiam resultar em um sistema impossível. De fato, foi uma coincidência que nossa última conta foi $(-2)\cdot (-3) + (-6)$, que resultou em um zero. Neste exemplo, qualquer variação na segunda componente de $\vec{b}$ nos daria um sistema impossível:
		\[
		\left[
		\begin{array}{ccc|c}
		1 & -3 & 0 & 2 \\
		0 &  4 & 1 & 1 \\
		3 & -1 & 2 & 0 \\
		\end{array}
		\right]
		\xrightarrow{-3\ell_1 + \ell_3 \text{ em } \ell_3}
		\left[
		\begin{array}{ccc|c}
		1 & -3 & 0 & 2 \\
		0 &  4 & 1 & 1 \\
		0 &  8 & 2 & -6 \\
		\end{array}
		\right] \xrightarrow{-2\ell_2 + \ell_3 \text{ em } \ell_3}
		\left[
		\begin{array}{ccc|c}
		1 & -3 & 0 & 2 \\
		0 &  4 & 1 & 1 \\
		0 &  0 & 0 & -8 \\
		\end{array}
		\right].
		\]
		\item É um fato geral: se \emph{nem todas} as colunas de $A$ possuem posição de pivô, podemos encontrar vetores $\vec{b}$ de modo que o sistema seja impossível e vetores $\vec{b}$ cujo sistema possua infinitas soluções;
		\item A forma escalonada reduzida de $A$ não é a identidade, isto é, $A$ não é equivalente por linhas a $I_3$. Em particular, $A$ não é invertível.
	\end{itemize} Vamos reescrever estas conclusões como no exemplo anterior:
	\begin{itemize}
		\item Existe $\vec{b}_1 \in \bR^3$ tal que o sistema $A \vec{x} = \vec{b}_1$ possui infinitas soluções e existe $\vec{b}_2 \in \bR^3$ tal que o sistema $A \vec{x} = \vec{b}_2$ possui infinitas soluções;
		\item Nem todo vetor $\vec{b} \in \bR^3$ pode ser escrito como combinação linear das colunas $\vec{v}_1, \vec{v}_2, \vec{v}_3$ de $A$;
		\item Existe $\vec{b} \in \bR^3$ não pertencente a $\Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$;
		\item $\Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\} \neq \bR^3$;
		\item $A$ não é equivalente por linhas à matriz identidade $I_3$;
		\item $A$ não é invertível;
		\item A imagem da transformação linear $T(\vec{x}) = A \vec{x}$ não é todo o espaço $\bR^3$, isto é, $T$ não é sobrejetora;
		\item A equação $T(\vec{x}) = \vec{b}$ pode possuir mais de uma solução, de modo que $T$ não é injetora;
		\item $T$ não é invertível, já que não é injetora e nem sobrejetora.  $\ \lhd$
	\end{itemize}
\end{example}


\subsection{Caracterizações de matrizes invertíveis}


A discussão feita na seção anterior é válida para matrizes quadradas de forma geral e vamos enunciar isto em um grande Teorema. A prova deste resultado abaixo segue as mesmas ideias dos exemplos da seção anterior. Não faremos a prova, mas é importante que o leitor esteja familiarizado com estas ideias.


\begin{theorem}
	Seja $A$ uma matriz quadrada de ordem $n\times n$. As afirmações abaixo são equivalentes (ou todas falsas ou todas verdadeiras):
	\begin{enumerate}
		\item $A$ é invertível;
		\item Existe uma matriz $B$ tal que $AB = I_n$;
		\item Existe uma matriz $C$ tal que $CA = I_n$;
		\item $A$ é equivalente por linhas à matriz identidade $I_n$;
		\item $A$ tem $n$ posições de pivô;
		\item Para todo $\vec{b} \in \bR^3$, o sistema $A \vec{x} = \vec{b}$ possui exatamente uma solução;
		\item O sistema $A \vec{x} = \vec{0}$ possui somente a solução trivial;
		\item As colunas $\vec{v}_1, \vec{v}_2, \vec{v}_3$ de $A$ são linearmente independentes;
		\item Todo vetor $\vec{b} \in \bR^3$ pode ser escrito como combinação linear das colunas $\vec{v}_1, \vec{v}_2, \vec{v}_3$ de $A$;
		\item Todo vetor $\vec{b} \in \bR^3$ pertence a $\Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\}$, espaço gerado pelas colunas de $A$;
		\item As colunas de $A$ geram $\bR^n$: $\Span\{\vec{v}_1, \vec{v}_2, \vec{v}_3\} = \bR^3$;
		\item $T(\vec{x}) = A \vec{x}$ é sobrejetora;
		\item $T(\vec{x}) = A \vec{x}$ é injetora;
		\item $T(\vec{x}) = A \vec{x}$ é invertível,
		\item $A^T$ é invertível.
	\end{enumerate}
\end{theorem}

\begin{exercise}
	Justificar, baseando-se nos exemplos da seção anterior, o porquê das afirmações acima serem equivalentes.
\end{exercise}

O teorema é utilizado na prática da mesma maneira que fizemos na seção anterior.




\section{Espaços vetoriais}


Um espaço vetorial (sobre o conjunto $\bR$ de escalares) é um conjunto $V$ equipado com as operações de soma de vetores e de multiplicação por escalar e que satisfazem as propriedades usuais dos espaços $\bR^n$. A ideia é que vários conjuntos mais abstratos possuem a estrutura parecida com a dos espaços $\bR^n$ e esta abordagem permite que façamos uma análise sistemática de todos estes casos.

De forma mais precisa, um \textbf{espaço vetorial sobre $\bR$} é um conjunto $V$, cujos elementos são chamados vetores, equipado com duas operações:
\begin{itemize}
	\item Soma entre vetores, que satisfaz
	\begin{enumerate}
		\item Associatividade: $(u+v)+w=u+(v+w)$, para quaisquer $u,v,w \in V$;
		\item Elemento neutro: existe o vetor $0 \in V$ que satisfaz $v+0=0+v=v$, para qualquer $v \in V$;
		\item Inverso aditivo: para cada $v \in V$, existe o inverso $u= -v \in V$, que satisfaz $v+u=0$;
		\item Comutatividade: $u+v = v+u$, para quaisquer $u, v \in V$;
	\end{enumerate}
	\item Multiplicação de vetor por escalar, que satisfaz
	\begin{enumerate}
		\item[5.] Associatividade da multiplicação por escalar: $a\cdot (b\cdot v)=(a\cdot b)\cdot v$, para quaisquer $a,b \in \bR$ e qualquer $v \in V$;
		\item[6.] Vale que $1 \cdot v = v$, ou seja, a unidade dos números reais não altera os vetores de $V$;
		\item[7.] Distributiva de um escalar em relação à soma de vetores: $a \cdot (u+v) = a\cdot v+a\cdot u$, para qualquer $a \in \bR$ e quaisquer $u,v \in V$;
		\item[8.] Distributiva da soma de escalares em relação a um vetor: $(a+b) \cdot v = a \cdot v+b \cdot v$, para quaisquer $a,b \in \bR$ e qualquer $v \in V$.
	\end{enumerate}
\end{itemize}

\begin{example}
	Naturalmente, $\bR$, $\bR^2$, $\bR^3$, ..., $\bR^n$ são espaços vetoriais sobre $\bR$, já que as propriedades acima foram todas inspiradas nas propriedades válidas para vetores de $\bR^n$.
\end{example}


\begin{example}
	O conjunto $M_{m\times n}$ de todas as matrizes de ordem $m\times n$ é um espaço vetorial, com as operações naturais que já utilizamos anteriormente: Se
	\[
	A =
	\left[
	\begin{array}{cccc}
	a_{11} & a_{12} & \cdots & a_{1n} \\
	a_{21} & a_{22} & \cdots & a_{2n} \\
	\vdots & \vdots &        & \vdots \\
	a_{m1} & a_{m2} & \cdots & a_{mn} \\
	\end{array}
	\right] \quad \text{e} \quad
	B =
	\left[
	\begin{array}{cccc}
	b_{11} & b_{12} & \cdots & b_{1n} \\
	b_{21} & b_{22} & \cdots & b_{2n} \\
	\vdots & \vdots &        & \vdots \\
	b_{m1} & b_{m2} & \cdots & b_{mn} \\
	\end{array}
	\right],
	\] definimos
	\[
	A + B =
	\left[
	\begin{array}{cccc}
	a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1n} + b_{1n} \\
	a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n} \\
	\vdots & \vdots &        & \vdots \\
	a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn} \\
	\end{array}
	\right] \quad \text{e} \quad
	k\cdot A =
	\left[
	\begin{array}{cccc}
	k\cdot a_{11} & k\cdot a_{12} & \cdots & k\cdot a_{1n} \\
	k\cdot a_{21} & k\cdot a_{22} & \cdots & k\cdot a_{2n} \\
	\vdots & \vdots &        & \vdots \\
	k\cdot a_{m1} & k\cdot a_{m2} & \cdots & k\cdot a_{mn} \\
	\end{array}
	\right].
	\] É imediato verificar que valem todas as 8 propriedades acima (faça!).
\end{example}


\begin{example}\label{funcoes}
  O conjunto de todas as funções de um intervalo $I$ em $\bR$
  \begin{equation}
    \mathcal{F} (I;\bR) = \left\{ f: I \to \bR \text{funções} \right\}.  
  \end{equation}
  forma um espaço vetorial sobre $\bR$, com as operações:
  \begin{itemize}
  \item Dadas duas funções $f$ e $g$, a função $f+g : I \to \bR$ é definida por
    \[
    \big( f+g \big) (x) = f(x) + g(x).
    \]
  \item Dada uma função $f$ e um número real $k$, a função $k \cdot f : I \to \bR$ é definida por
    \[
    \big( k \cdot f \big) (x) = k \cdot f(x).
    \]
  \end{itemize}
\end{example}


\begin{exercise}
	Justifique que o espaço gerado por dois elementos $\Span \{ \vec{u}, \vec{v} \}$ é um espaço vetorial.
\end{exercise}


\subsection{Subespaços vetoriais}

Consideramos $V$ um espaço vetorial e $H\subseteq V$ um subconjunto. Nós dizemos que $H$ é uma \textbf{subespaço vetorial de} $V$ se, além de ser um subconjunto, $H$ é por si só um espaço vetorial.

Na prática, para decidir se um subconjunto $H$ é subespaço de $V$, não precisamos verificar todas as oito propriedades da definição de espaço vetorial. Temos o seguinte:

%\begin{theorem}[Critério para subespaços]
\begin{theorem}
	Um subconjunto $H \subseteq V$ é subespaço de $V$ se, e somente se, as duas propriedades abaixo são verificadas:
	\begin{itemize}
		\item $0 \in H$;
		\item $a\cdot u + b \cdot v \in V$ para quaisquer $a, b \in \bR$ e quaisquer $u,v \in V$.
	\end{itemize}
\end{theorem}


\begin{example}\label{reta}
	A reta $H_1 = \{y = 2x\}$ em $\bR^2$ é um subespaço de $\bR^2$. Temos que
	\begin{itemize}
		\item $\vec{0} = (0,0)$ pertence a reta, pois satisfaz a equação da reta: $0 = 2\cdot 0$;
		\item se $(x_1, y_1)$ e $(x_2, y_2)$ pertencem à reta, vamos verificar que $(ax_1 + b x_2, ay_1 + b y_2)$ também pertence:
		\[
		ay_1 + b y_2 = a \cdot (2x_1) + b\cdot (2x_2) = 2 (ax_1 + b x_2).
		\] Portanto, $H$ é um subespaço de $\bR^2$.
	\end{itemize}
\end{example}

\begin{example}
	A reta $H_2 = \{y = 2x + 3\}$ \underline{não} é um subespaço de $\bR^2$. De fato, $\vec{0} = (0,0)$ não pertence a reta. De um modo geral, as retas que passam pela origem são subespaços e as retas que não passam pela origem não são.
\end{example}


\begin{example}
	Mais geralmente, para $\vec{u}, \vec{v} \in \bR^n$, o espaço gerado $\Span \{\vec{u}, \vec{v}\}$ é um subespaço de $\bR^n$. Caso os vetores sejam LI, este espaço pode ser pensado como o plano que contém os vetores $\vec{u}, \vec{v}$ dentro de $\bR^n$ (por quê?).
	
	Poderíamos argumentar que a reta do Exemplo \ref{reta} é um subespaço, pois todos os pontos da reta satisfazem:
	\[
	\left[
	\begin{array}{c}
	x \\
	y \\
	\end{array}
	\right] =
	\left[
	\begin{array}{c}
	x \\
	2x \\
	\end{array}
	\right] = x \cdot
	\left[
	\begin{array}{c}
	1 \\
	2 \\
	\end{array}
	\right],
	\] isto é, $H = \Span \left\{ \left[
	\begin{array}{c}
	1 \\
	2 \\
	\end{array}
	\right] \right\}$.
\end{example}


\begin{example}
	Um plano que passa pela origem $2x - y + 3z = 0$ é um subespaço de $\bR^3$, pois podemos escrever pontos do plano como
	\[
	\left[
	\begin{array}{c}
	x \\
	y \\
	z \\
	\end{array}
	\right] =
	\left[
	\begin{array}{c}
	x \\
	2x +3z \\
	z \\
	\end{array}
	\right] = x \cdot
	\left[
	\begin{array}{c}
	1 \\
	2 \\
	0 \\
	\end{array}
	\right] + z \cdot
	\left[
	\begin{array}{c}
	0 \\
	3 \\
	1 \\
	\end{array}
	\right],
	\] isto é,
	\[
	H = \Span \left\{
	\left[
	\begin{array}{c}
	1 \\
	2 \\
	0 \\
	\end{array}
	\right],
	\left[
	\begin{array}{c}
	0 \\
	3 \\
	1 \\
	\end{array}
	\right]
	\right\}
	\] Por outro lado, um plano que não passa pela origem, eg, $2x - y + 3z = 1$, não pode ser um subespaço, pois $\vec{0} \not\in H$.
\end{example}



\begin{example}
	O conjunto $\cP (\bR)$ das funções polinomiais com coeficientes reais é um subespaço do espaço das funções contínuas do Exemplo \ref{funcoes}. De fato, o polinômio nulo está neste espaço. Além disso, dados dois polinômios
	\[
	p(x) = a_n x^n + \cdots a_2 x^2 + a_1 x + a_0 \quad \text{e} \quad q(x) = b_m x^m + \cdots b_2 x^2 + b_1 x + b_0,
	\] sabemos que $a \cdot p + b \cdot q$ vai ser um polinômio cujo grau é no máximo o maior dos valores $m$ ou $n$.
\end{example}


\begin{exercise}
	Considere o conjunto
	\[
	\cP_3 (\bR) = \{ p(x) = a_3 x^3 + \cdots a_2 x^2 + a_1 x + a_0 \text{ tais que } a_0, a_1, a_2, a_3 \in \bR \}
	\] o conjunto de todos os polinômios de grau 3. Mostre que:
	\begin{enumerate}
		\item $\cP_3 (\bR)$ é um subespaço de $\cP (\bR)$;
		\item $\cP_3 (\bR)$ é um subespaço de $\cF (I;\bR)$.
	\end{enumerate} É possível generalizar estas conclusões para o conjunto $\cP_n (\bR)$ dos polinômios de grau $n$?
\end{exercise}


\subsection{Espaço nulo}

O \textbf{espaço nulo} de uma matriz $A$ de ordem $m \times n$ é o conjunto definido por
\[
\Nul A = \big\{ \vec{x} \in \bR^n \, | \, A\vec{x} = \vec{0}  \big\}.
\] Para verificar que $\Nul A$ é um subespaço de $\bR^n$, consideramos $\vec{u}, \vec{v} \in \Nul A$ e $a,b \in \bR$. Queremos ver que $a\vec{u} + b\vec{v}$ também pertence a $\Nul A$: Pela linearidade do produto de matrizes, temos
\[
A (a\vec{u} + b\vec{v}) = a \cdot A\vec{u} + b\cdot A\vec{v} = a\cdot \vec{0} + b \cdot \vec{0} = \vec{0}.
\] Portanto, como também $\vec{0} \in \Nul A$, concluimos que $\Nul A$ é subespaço de $\bR^n$.

\begin{example}[Retirado do David C. Lay]
	Encontrar um conjunto gerador para o espaço nulo da matriz
	\[
	\left[
	\begin{array}{ccccc}
	-3 & 6  & -1 & 1 & -7 \\
	1  & -2 & 2  & 3 & -1 \\
	2  & -4 & 5  & 8 & -4 \\
	\end{array}
	\right]
	\] Nós queremos determinar as soluções do sistema linear homogêneo
	\[
	\left[
	\begin{array}{ccccc}
	-3 & 6  & -1 & 1 & -7 \\
	1  & -2 & 2  & 3 & -1 \\
	2  & -4 & 5  & 8 & -4 \\
	\end{array}
	\right]
	\left[
	\begin{array}{c}
	x_1 \\
	x_2 \\
	x_3 \\
	x_4 \\
	x_5 \\
	\end{array}
	\right] = 
	\left[
	\begin{array}{c}
	0 \\
	0 \\
	0 \\
	\end{array}
	\right].
	\] Poderíamos escrever a matriz associada aumentada e escalonar, mas como este é um sistema homogêneo, não faz diferença escrevermos a última coluna com os zeros ou não (nenhuma operação elementar fará com que desapareçam os zeros). Logo, vamos obter a forma escalonada reduzida da matriz $A$ (contas como exercício):
	\[
	\left[
	\begin{array}{ccccc}
	-3 & 6  & -1 & 1 & -7 \\
	1  & -2 & 2  & 3 & -1 \\
	2  & -4 & 5  & 8 & -4 \\
	\end{array}
	\right] \sim 
	\left[
	\begin{array}{ccccc}
	1 & -2 & 0  & -1 & 3  \\
	0 & 0  & 1  & 2  & -2 \\
	0 & 0  & 0  & 0  & 0  \\
	\end{array}
	\right] \sim 
	\left\{
	\begin{array}{ll}
	x_1 - 2x_2 - x_4 + 3x_5 = 0 \\
	x_3 + 2 x_4 -2 x_5 = 0 \\
	x_2, x_4, x_5 \hbox{ livres.}
	\end{array}
	\right.
	\] Assim, podemos escrever qualquer vetor de $\Nul A$ como
	\[
	\left[
	\begin{array}{c}
	x_1 \\
	x_2 \\
	x_3 \\
	x_4 \\
	x_5 \\
	\end{array}
	\right] = 
	\left[
	\begin{array}{c}
	2x_2 + x_4 - 3x_5 \\
	x_2 \\
	- 2 x_4 + 2 x_5 \\
	x_4 \\
	x_5 \\
	\end{array}
	\right] = x_2 
	\left[
	\begin{array}{c}
	2 \\
	1 \\
	0 \\
	0 \\
	0 \\
	\end{array}
	\right] + x_4
	\left[
	\begin{array}{c}
	1 \\
	0 \\
	-2 \\
	1 \\
	0 \\
	\end{array}
	\right] + x_5
	\left[
	\begin{array}{c}
	-3 \\
	0 \\
	2 \\
	0 \\
	1 \\
	\end{array}
	\right].
	\] Isto significa que 
	\[
	\Nul A = 
	\Span \left\{
	\left[
	\begin{array}{c}
	2 \\
	1 \\
	0 \\
	0 \\
	0 \\
	\end{array}
	\right],
	\left[
	\begin{array}{c}
	1 \\
	0 \\
	-2 \\
	1 \\
	0 \\
	\end{array}
	\right],
	\left[
	\begin{array}{c}
	-3 \\
	0 \\
	2 \\
	0 \\
	1 \\
	\end{array}
	\right]\right\}. \ \lhd
	\] 
\end{example}

Uma das importâncias teóricas do espaço nulo é

\begin{proposition}
	Seja $A$ uma matriz $m\times n$. Então a aplicação linear $\vec{x} \mapsto A \vec{x}$ é injetora se, e somente se, $\Nul A = \{ 0 \}.$
\end{proposition}

Em outras palavras, caso o sistema linear homogêneo $A \vec{x} = \vec{0}$ possua somente a solução trivial, podemos inferir que $A \vec{x} = \vec{b}$ possui no máximo uma solução, para qualquer $\vec{b}$!

\subsection{Espaço coluna}


O \textbf{espaço coluna} de uma matriz $A$ de ordem $m \times n$ é o espaço gerado pelas colunas de $A$:
\[
A =
\left[
\begin{array}{cccc}
| & | &  & | \\
\vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \\
| & | &        & | \\
\end{array}
\right] \rightsquigarrow
\Col A = \Span \{ \vec{a}_1, \vec{a}_2, \dots, \vec{a}_n\}.
\]

\begin{exercise}
	Verificar que $\Col A$ é um subespaço de $\bR^n$.
\end{exercise}

Lembrando ainda que
\[
x_1 \vec{a}_1 + x_2 \vec{a}_2 + \cdots + x_n \vec{a}_n = \vec{b} \iff A \vec{x} = \vec{b},
\] podemos reescrever a definição como 
\[
\Col A = \{ \vec{b} \in \bR^m \ | \ \text{existe } \vec{x} \text{ tal que } \vec{b} = A \vec{x} \}.
\] O espaço coluna trata portanto de vetores na imagem da aplicação linear. Temos:

\begin{proposition}
	Seja $A$ uma matriz $m\times n$. Então a aplicação linear $\vec{x} \mapsto A \vec{x}$ é sobrejetora se, e somente se, $\Col A = \bR^m.$
\end{proposition}

A definição do espaço coluna já deixa claro um conjunto gerador (as colunas de $A$!). Não precisamos fazer cálculos como fizemos para encontrar geradores para $\Nul A$.

\end{document} 